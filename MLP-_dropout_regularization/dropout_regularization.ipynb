{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see some examples of using dropout layers and regularization in a Neural Network model. Here, we will not train the model but we just see how to use dropout in a Neural Network. In future posts, I explain the concept behind Neural Networks and we will train the network over a dataset to see the performance with/without dropout layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Using Dropout in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = number_of_epochs\n",
    "input_size = number_of_input_neurons\n",
    "outpot_size = _number_of_output_neurons\n",
    "activation_func = activation_function\n",
    "loss_func = binary_crossentropy\n",
    "num_neurons = number_of_neurons_for_the_layer\n",
    "batch_size = number_of_batch_size\n",
    "X_train = training_data\n",
    "y_train = training_labels\n",
    "X_valid = validation_data\n",
    "y_valid = validation_labels\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(\n",
    "    Dense(units=num_neurons, activation=activation_func, input_dim=input_size))\n",
    "classifier.add(Dropout(0.2))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units=num_neurons, activation=activation_func))\n",
    "classifier.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units=outpot_size, activation=activation_func)\n",
    "\n",
    "# initialize our optimizer and model, then compile it\n",
    "opt=optimizer\n",
    "\n",
    "# Compiling the ANN\n",
    "METRICS=[\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'),\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'),\n",
    "]\n",
    "\n",
    "loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "    name=loss_func\n",
    ")\n",
    "classifier.compile(optimizer=opt, loss=loss, metrics=METRICS)\n",
    "\n",
    "early_stopping=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_prc',\n",
    "    verbose=1,\n",
    "    patience=15,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "my_callbacks=[early_stopping]\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_valid, y_valid),\n",
    "                         epochs=num_epoch, verbose=1, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Using l2 regularization in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = number_of_epochs\n",
    "input_size = number_of_input_neurons\n",
    "outpot_size = _number_of_output_neurons\n",
    "activation_func = activation_function\n",
    "loss_func = binary_crossentropy\n",
    "num_neurons = number_of_neurons_for_the_layer\n",
    "batch_size = number_of_batch_size\n",
    "X_train = training_data\n",
    "y_train = training_labels\n",
    "X_valid = validation_data\n",
    "y_valid = validation_labels\n",
    "opt = optimizer\n",
    "\n",
    "METRICS = [\n",
    "    keras.metrics.TruePositives(name=\"tp\"),\n",
    "    keras.metrics.FalsePositives(name=\"fp\"),\n",
    "    keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "    keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "    keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "    keras.metrics.Precision(name=\"precision\"),\n",
    "    keras.metrics.Recall(name=\"recall\"),\n",
    "    keras.metrics.AUC(name=\"auc\"),\n",
    "    keras.metrics.AUC(name=\"prc\", curve=\"PR\"),\n",
    "]\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_prc\", verbose=1, patience=15, mode=\"max\", restore_best_weights=True\n",
    ")\n",
    "my_callbacks = [early_stopping]\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "classifier.add(\n",
    "    Dense(\n",
    "        num_neurons,\n",
    "        input_dim=input_size,\n",
    "        activation=activation_func,\n",
    "        kernel_regularizer=\"l2\",\n",
    "    )\n",
    ")\n",
    "# Adding the input layer and the first hidden layer along with the l2 regularization\n",
    "classifier.add(Dense(num_neurons, activation=activation_func, kernel_regularizer=\"l2\"))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units=outpot_size, activation=activation_func)\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss=loss_func, optimizer=opt, metrics=METRICS)\n",
    "\n",
    "# Training the model\n",
    "classifier_history = classifier.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=num_epoch,\n",
    "    verbose=1,\n",
    "    callbacks=my_callbacks,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
